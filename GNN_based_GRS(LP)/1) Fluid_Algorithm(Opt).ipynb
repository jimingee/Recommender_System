{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3455eeb1",
   "metadata": {},
   "source": [
    "## Fluid_Algorithm\n",
    "\n",
    "Parés F., Garcia-Gasulla D. et al. “Fluid Communities: A Competitive and Highly Scalable Community Detection Algorithm”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73166f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "import networkx as nx\n",
    "from networkx import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6a91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80000row\n",
    "user_train = pd.read_csv('ml-100k/u1.base', sep='\\t',names=[\"userID\",\"itemID\",\"rating\",\"timestamp\"],header=None, na_filter=False)\n",
    "user_train = user_train[['userID','itemID','rating']]\n",
    "\n",
    "# 100000row\n",
    "user_total = pd.read_csv('ml-100k/u.data', sep='\\t',names=[\"userID\",\"itemID\",\"rating\",\"timestamp\"],header=None, na_filter=False)\n",
    "user_total = user_total[['userID','itemID','rating']]\n",
    "\n",
    "# 20000row\n",
    "user_test = pd.read_csv('ml-100k/u1.test', sep='\\t',names=[\"userID\",\"itemID\",\"rating\",\"timestamp\"],header=None, na_filter=False)\n",
    "user_test = user_test[['userID','itemID','rating']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d10036b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmin_max_scaler = MinMaxScaler()\\n\\nx_scaled = min_max_scaler.fit_transform(user_train[['rating']])\\nuser_train[['rating']] = x_scaled\\n\\nx_scaled = min_max_scaler.fit_transform(user_total[['rating']])\\nuser_total[['rating']] = x_scaled\\n\\nx_scaled = min_max_scaler.fit_transform(user_test[['rating']])\\nuser_test [['rating']] = x_scaled\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(user_train[['rating']])\n",
    "user_train[['rating']] = x_scaled\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(user_total[['rating']])\n",
    "user_total[['rating']] = x_scaled\n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(user_test[['rating']])\n",
    "user_test [['rating']] = x_scaled\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137c3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_total = user_total.pivot_table('rating', index = 'userID',columns = 'itemID').fillna(0)\n",
    "user_item_train = user_train.pivot_table('rating', index = 'userID',columns = 'itemID').fillna(0)\n",
    "user_item_test = user_test.pivot_table('rating', index = 'userID',columns = 'itemID').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bd618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u196</td>\n",
       "      <td>i242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u186</td>\n",
       "      <td>i302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u22</td>\n",
       "      <td>i377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u244</td>\n",
       "      <td>i51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u166</td>\n",
       "      <td>i346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>u880</td>\n",
       "      <td>i476</td>\n",
       "      <td>3</td>\n",
       "      <td>880175444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>u716</td>\n",
       "      <td>i204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>u276</td>\n",
       "      <td>i1090</td>\n",
       "      <td>1</td>\n",
       "      <td>874795795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>u13</td>\n",
       "      <td>i225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>u12</td>\n",
       "      <td>i203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID itemID  rating  timestamp\n",
       "0       u196   i242       3  881250949\n",
       "1       u186   i302       3  891717742\n",
       "2        u22   i377       1  878887116\n",
       "3       u244    i51       2  880606923\n",
       "4       u166   i346       1  886397596\n",
       "...      ...    ...     ...        ...\n",
       "99995   u880   i476       3  880175444\n",
       "99996   u716   i204       5  879795543\n",
       "99997   u276  i1090       1  874795795\n",
       "99998    u13   i225       2  882399156\n",
       "99999    u12   i203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## user-item node 이름 설정\n",
    "matrix = pd.read_csv('ml-100k/u.data', sep='\\t',names=[\"userID\",\"itemID\",\"rating\",\"timestamp\"],header=None, na_filter=False)\n",
    "matrix[['userID']] = 'u' + matrix[['userID']].astype(str)\n",
    "matrix[['itemID']] = 'i' + matrix[['itemID']].astype(str)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522e327",
   "metadata": {},
   "source": [
    "### 그래프 생성\n",
    "\n",
    "#### G : user로만 이루어진 그래프  /  B : user, item으로 이루어진 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95902d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_node = matrix[['userID']].values\n",
    "user_node = np.array(user_node).flatten().tolist()\n",
    "#user_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3325052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph with 943 nodes and 0 edges'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(user_node)\n",
    "info(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da03e55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph with 2625 nodes and 100000 edges'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist = []\n",
    "\n",
    "for i in matrix.values:\n",
    "    edgelist.append((i[0],i[1]))\n",
    "    \n",
    "B = nx.Graph() # >300\n",
    "B.add_nodes_from(matrix.userID, bipartite=0) # user\n",
    "B.add_nodes_from(matrix.itemID, bipartite=1) # item(movie)\n",
    "B.add_edges_from(edgelist)\n",
    "\n",
    "info(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd47d1",
   "metadata": {},
   "source": [
    "###  Link Prediction으로 상위 0.5 % 가능성을 가진  edge 생성\n",
    "\n",
    "common_neighbor_centrality(2020) 알고리즘 사용\n",
    "\n",
    "* uesr graph 연결될 수 있는 최대 edge 개수 444153 [ = 943*942/2 ]\n",
    "* total graph 연결될 수 있는 최대 edge 개수 3444000 [ = 2625*2624/2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cef68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_graph connected :  True  user graph info :  Graph with 2625 nodes and 116425 edges\n",
      "user_graph connected :  False  user graph info :  Graph with 943 nodes and 8702 edges\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    user_edgelist = []\n",
    "    total_edgelist = []\n",
    "    \n",
    "    pred_link = list(nx.common_neighbor_centrality(B, alpha=0.8))\n",
    "    \n",
    "    link_likelihood = pd.DataFrame(pred_link).iloc[:,2].values\n",
    "    link_99 = np.percentile(link_likelihood,  99.5, interpolation='linear')\n",
    "    \n",
    "    for p in pred_link:\n",
    "        edge = list(p)\n",
    "        linklihood = int(edge[2])\n",
    "        \n",
    "        if linklihood > link_99:\n",
    "            if edge[0][:1] ==  edge[1][:1] == 'u': # user R\n",
    "                user_edgelist.append((edge[0], edge[1]))\n",
    "                total_edgelist.append((edge[0], edge[1]))\n",
    "            else :\n",
    "                total_edgelist.append((edge[0], edge[1]))\n",
    "                \n",
    "    B.add_edges_from(total_edgelist)\n",
    "    G.add_edges_from(user_edgelist)\n",
    "    \n",
    "    print('total_graph connected : ', is_connected(B),' user graph info : ', info(B))\n",
    "    print('user_graph connected : ', is_connected(G),' user graph info : ', info(G))\n",
    "    \n",
    "    ## total graph가 다 연결될 때 까지 반복\n",
    "    if is_connected(B):\n",
    "        break\n",
    "    \n",
    "#print('total graph: ', info(B))\n",
    "#print('user_graph connected : ', is_connected(G),' user graph info : ', info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a1702",
   "metadata": {},
   "source": [
    "###  Graph Clustering\n",
    "\n",
    "\n",
    "변수명 설명\n",
    "* cluster : 각 그룹 넘버와 그룹에 해당되는 user, item 리스트\n",
    "* [return] user_c : 각 user 당 해당하는 cluster number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2727c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## network clustering\n",
    "from networkx.algorithms.community import asyn_fluidc\n",
    "from networkx.algorithms.community import coverage, performance\n",
    "\n",
    "coverage_list, performance_list = [], [] \n",
    "\n",
    "def graph_clustering(cluster_num):\n",
    "    cluster = [0] * cluster_num\n",
    "    total_graph = B\n",
    "    \n",
    "    community = list(asyn_fluidc(total_graph, k=cluster_num))\n",
    "    \n",
    "    for i, comms in enumerate(community):\n",
    "        cluster[i] = comms\n",
    "        \n",
    "    coverage_list.append(coverage(total_graph, community))\n",
    "    performance_list.append(performance(total_graph, community))\n",
    "\n",
    "                \n",
    "    ## user cluster 정보\n",
    "    cluster_id = pd.read_csv('ml-100k/u.user', sep='|',names=[\"userID\",\"age\",\"gender\",\"occupation\",\"zip code\"],header=None,na_filter=False)\n",
    "    cluster_id = cluster_id[['userID']]\n",
    "    cluster_id= cluster_id.set_index('userID')\n",
    "    cluster_id['cluster'] = 999\n",
    "\n",
    "    cluster_cnt = [] # 각 cluster에 속한 user의 수 \n",
    "    \n",
    "    # 각 user와 cluster matching\n",
    "    for i in range(cluster_num):\n",
    "        cnt = 0\n",
    "        \n",
    "        for j in list(cluster[i]):\n",
    "            if j[0] == 'u':\n",
    "                cnt += 1\n",
    "                cluster_id.iloc[int(j[1:])-1] = i \n",
    "                \n",
    "        cluster_cnt.append(cnt)\n",
    "            \n",
    "    # 각 클러스터당 user의 인원 수\n",
    "    #print('cluster num : ', cluster_num, \" -> \",cluster_cnt) \n",
    "    #print(user_c)\n",
    "    \n",
    "    return (cluster_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728fe6e",
   "metadata": {},
   "source": [
    "###  Group Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fec5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def grs_ndcg(num, total_matrix, train_matrix):\n",
    "    total_matrix  # user_item_total \n",
    "    train_matrix  # user_item_train\n",
    "    test_matrix = user_item_test\n",
    "    \n",
    "    ## 1. fluid algorithm으로 그룹 clustering(total 대상)\n",
    "    cluster_id = graph_clustering(num)\n",
    "    \n",
    "    # 각 클러스터에 해당하는 개수\n",
    "    length = [1]*num\n",
    "    for i in range(num):\n",
    "        length[i] += len(cluster_id[cluster_id.cluster==i]) \n",
    "    \n",
    "    \n",
    "    # train, test 에 cluster 정보 추가\n",
    "    user_item_train_cl = pd.concat([train_matrix, cluster_id], axis=1, join='inner')\n",
    "    user_item_test_cl = pd.concat([test_matrix, cluster_id], axis=1, join='inner')\n",
    "    \n",
    "    ## 2. 클러스터 별로 각 item의 mean 값 구함 (train 대상)\n",
    "    mean_rating = pd.DataFrame(columns = user_item_train_cl.columns)\n",
    "    mean_rating.set_index('cluster')\n",
    "    \n",
    "    for i in range(num):\n",
    "        mean_rating = mean_rating.append(user_item_train_cl[user_item_train_cl.cluster == i].mean(axis=0), ignore_index=True)\n",
    "    \n",
    "    mean_rating = mean_rating.set_index('cluster')\n",
    "    mean_rating\n",
    "    \n",
    "    ## 3. train-test set의 columns(item id) 맞추기 (miss matching 제거)\n",
    "    for c in user_item_train_cl.columns:\n",
    "        if c not in user_item_test_cl.columns:\n",
    "            del mean_rating[c]\n",
    "        \n",
    "    for c in user_item_test_cl.columns:\n",
    "        if c not in user_item_train_cl.columns:\n",
    "            del user_item_test_cl[c] \n",
    "            \n",
    "    y_pred = mean_rating \n",
    "    y_true = user_item_test_cl\n",
    "    \n",
    "    result = [0]*num # 결과값 저장 리스트\n",
    "    \n",
    "    ## 4. 각 결과 값에 nDCG 더해줌\n",
    "    for idx in test_matrix.index:\n",
    "        cluster_num = int(y_true.loc[idx].cluster)\n",
    "        result[cluster_num] += ndcg_score([y_true.loc[idx][:-1]], [y_pred.loc[cluster_num]])\n",
    "        #result[cluster] += ndcg_score([user_item_test_cl.loc[idx][:-1]], [mean_rating.loc[cluster]], k=4)\n",
    "    \n",
    "    ## 5. 최종적으로 각 nDCG값 / 각 cluster의 요소 개수\n",
    "    for i in range(num):\n",
    "        result[i] = result[i]/length[i]\n",
    "        \n",
    "    print(length)\n",
    "    \n",
    "    #print(\"cluster수:\",len(length),\"/ NDCG:\",sum(result)/len(length))  \n",
    "    print('%.5f'%(sum(result)/(len(length))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0646f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[651, 294]\n",
      "0.24735\n",
      "[520, 149, 277]\n",
      "0.25562\n",
      "[548, 19, 297, 83]\n",
      "0.25119\n",
      "[14, 402, 256, 273, 3]\n",
      "0.24426\n",
      "[369, 31, 437, 71, 38, 3]\n",
      "0.26463\n",
      "[20, 370, 40, 64, 211, 2, 243]\n",
      "0.24201\n",
      "[134, 247, 222, 299, 10, 4, 33, 2]\n",
      "0.22872\n",
      "[140, 252, 280, 3, 225, 10, 29, 11, 2]\n",
      "0.24533\n",
      "[7, 51, 259, 145, 9, 229, 218, 3, 29, 3]\n",
      "0.23591\n",
      "[161, 8, 354, 93, 75, 213, 14, 7, 18, 10, 1]\n",
      "0.19816\n",
      "[42, 345, 43, 9, 2, 185, 65, 215, 14, 30, 3, 2]\n",
      "0.23841\n",
      "[73, 134, 25, 91, 24, 238, 10, 2, 185, 166, 4, 2, 2]\n",
      "0.21094\n",
      "[327, 206, 3, 2, 14, 29, 2, 191, 67, 13, 51, 33, 12, 7]\n",
      "0.24415\n",
      "[168, 27, 312, 11, 51, 13, 2, 7, 210, 11, 72, 43, 17, 10, 4]\n",
      "0.21532\n",
      "[196, 95, 20, 280, 3, 5, 63, 182, 6, 2, 46, 20, 27, 5, 4, 5]\n",
      "0.21290\n",
      "[248, 148, 9, 161, 97, 7, 2, 54, 2, 72, 55, 41, 6, 23, 28, 5, 2]\n",
      "0.20892\n",
      "[22, 267, 9, 154, 2, 87, 21, 96, 18, 54, 24, 15, 167, 8, 4, 3, 3, 7]\n",
      "0.21445\n",
      "[24, 8, 10, 6, 2, 29, 16, 269, 154, 2, 179, 27, 63, 15, 51, 11, 83, 7, 6]\n",
      "0.21715\n",
      "[314, 39, 28, 34, 61, 10, 150, 3, 29, 10, 24, 169, 5, 16, 9, 6, 33, 17, 4, 2]\n",
      "0.22377\n",
      "[31, 274, 2, 151, 88, 25, 47, 20, 31, 37, 144, 7, 19, 21, 18, 15, 8, 5, 14, 4, 3]\n",
      "0.22228\n",
      "[2, 7, 8, 2, 26, 158, 12, 17, 300, 18, 11, 13, 64, 84, 14, 19, 21, 133, 14, 15, 26, 1]\n",
      "0.21014\n",
      "[154, 9, 130, 210, 146, 3, 26, 2, 15, 24, 12, 47, 7, 8, 47, 22, 13, 22, 31, 2, 32, 2, 2]\n",
      "0.22372\n",
      "[10, 48, 106, 4, 274, 2, 2, 2, 33, 2, 7, 151, 42, 51, 7, 9, 8, 26, 111, 20, 13, 12, 20, 7]\n",
      "0.20076\n",
      "[2, 10, 2, 64, 66, 273, 21, 10, 14, 3, 19, 32, 7, 24, 117, 18, 21, 144, 12, 2, 24, 50, 21, 8, 4]\n",
      "0.22978\n",
      "[104, 8, 4, 26, 6, 146, 50, 248, 68, 3, 10, 18, 5, 10, 3, 39, 74, 18, 9, 19, 51, 21, 8, 10, 7, 4]\n",
      "0.21391\n",
      "[209, 147, 2, 3, 25, 5, 43, 110, 14, 99, 2, 15, 5, 21, 9, 23, 5, 1, 24, 4, 38, 14, 14, 5, 21, 73, 39]\n",
      "0.22112\n",
      "[243, 45, 17, 9, 132, 9, 11, 108, 47, 7, 2, 28, 6, 6, 92, 27, 45, 29, 26, 11, 9, 9, 12, 8, 2, 2, 27, 2]\n",
      "0.21357\n",
      "[7, 38, 38, 22, 259, 95, 77, 7, 23, 133, 24, 10, 7, 2, 7, 38, 38, 3, 9, 15, 31, 17, 3, 12, 20, 5, 28, 2, 2]\n",
      "0.21482\n",
      "[6, 11, 21, 5, 147, 12, 25, 65, 8, 41, 18, 238, 9, 102, 24, 12, 12, 7, 3, 10, 57, 14, 12, 14, 5, 74, 11, 7, 2, 1]\n",
      "0.20831\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "cluster_num = range(2,31)\n",
    "#cluster_num = [2]\n",
    "\n",
    "for i in cluster_num:\n",
    "    grs_ndcg(i, user_item_total, user_item_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(coverage_list, 'ob--', label='coverage')\n",
    "plt.plot(performance_list, 'or--', label='performance')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39744fc2",
   "metadata": {},
   "source": [
    "## precision & recall : binary (2.5 이상 1, 나머지 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision & recall : binary (2.5 이상 1, 나머지 0)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def groupRS_pre_rec(num, total_matrix, train_matrix):\n",
    "    total_matrix  # user_item_total \n",
    "    train_matrix  # user_item_train\n",
    "    test_matrix = user_item_test\n",
    "    \n",
    "    ## 1. fluid algorithm으로 그룹 clustering(total 대상)\n",
    "    cluster_id = graph_clustering(num)\n",
    "    \n",
    "    # 각 클러스터에 해당하는 개수\n",
    "    length = [1]*num\n",
    "    for i in range(num):\n",
    "        length[i] += len(cluster_id[cluster_id.cluster==i]) \n",
    "    \n",
    "    \n",
    "    # train, test 에 cluster 정보 추가\n",
    "    user_item_train_cl = pd.concat([train_matrix, cluster_id], axis=1, join='inner')\n",
    "    user_item_test_cl = pd.concat([test_matrix, cluster_id], axis=1, join='inner')\n",
    "    \n",
    "    ## 2. 클러스터 별로 각 item의 mean 값 구함 (train 대상)\n",
    "    mean_rating = pd.DataFrame(columns = user_item_train_cl.columns)\n",
    "    mean_rating.set_index('cluster')\n",
    "    \n",
    "    for i in range(num):\n",
    "        mean_rating = mean_rating.append(user_item_train_cl[user_item_train_cl.cluster == i].mean(axis=0), ignore_index=True)\n",
    "    \n",
    "    mean_rating = mean_rating.set_index('cluster')\n",
    "    mean_rating\n",
    "    \n",
    "    ## 3. train-test set의 columns(item id) 맞추기 (miss matching 제거)\n",
    "    for c in user_item_train_cl.columns:\n",
    "        if c not in user_item_test_cl.columns:\n",
    "            del mean_rating[c]\n",
    "        \n",
    "    for c in user_item_test_cl.columns:\n",
    "        if c not in user_item_train_cl.columns:\n",
    "            del user_item_test_cl[c] \n",
    "            \n",
    "    y_pred = mean_rating \n",
    "    y_true = user_item_test_cl\n",
    "    \n",
    "    ## binary로 만들기 위해 2개의 분류로 생성\n",
    "    y_pred[y_pred< 2.5] = 0\n",
    "    y_pred[y_pred >= 2.5] = 1\n",
    "    \n",
    "    y_true[y_true< 2.5] = 0\n",
    "    y_true[y_true >= 2.5] = 1\n",
    "    \n",
    "    pre_result = [0]*num # 결과값 저장 리스트\n",
    "    rec_result = [0]*num\n",
    "    \n",
    "    ## 4. 각 결과 값에 precision, recall더해줌\n",
    "    for idx in test_matrix.index:\n",
    "        cluster_num = int(y_true.loc[idx].cluster)\n",
    "        pre_result[cluster_num] += precision_score(list(y_true.loc[idx][:-1].values) ,list(y_pred.loc[cluster_num].values ), average = 'binary')\n",
    "        rec_result[cluster_num] += recall_score(list(y_true.loc[idx][:-1].values) ,list(y_pred.loc[cluster_num].values ), average = 'binary')\n",
    "    \n",
    "    print(pre_result)\n",
    "    print(rec_result)\n",
    "    #print(sum(pre_result)/num)\n",
    "    \n",
    "    #print(sum(rec_result)/num)\n",
    "    ## 5. 최종적으로 각 precision, recall값 / 각 cluster의 요소 개수\n",
    "    #for i in range(num):\n",
    "    #    result[i] = result[i]/length[i]\n",
    "\n",
    "    \n",
    "    #print(\"cluster수:\",len(length),\"/ NDCG:\",sum(result)/len(length))  \n",
    "    print('%.5f'%(sum(pre_result)/num), \", \",  '%.5f'%(sum(rec_result)/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cluster_num = range(2,31)\n",
    "#cluster_num=[2]\n",
    "\n",
    "for i in cluster_num:\n",
    "    #reduced_total = SVD.fit_transform(user_item_total)\n",
    "    #reduced_total = pd.DataFrame(reduced_total)\n",
    "    #reduced_total.index = total_user_idx\n",
    "    \n",
    "    #user_item_train = pd.DataFrame(user_item_train)\n",
    "    \n",
    "    groupRS_pre_rec(i, user_item_total, user_item_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9da7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649db01e",
   "metadata": {},
   "source": [
    "## precision & recall : multiclass(반올림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba00867",
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision & recall : multiclass(반올림)\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import math\n",
    "\n",
    "def groupRS_pre_rec2(num, total_matrix, train_matrix):\n",
    "    total_matrix  # user_item_total \n",
    "    train_matrix  # user_item_train\n",
    "    test_matrix = user_item_test\n",
    "    \n",
    "    \n",
    "   ## 1. fluid algorithm으로 그룹 clustering(total 대상)\n",
    "    cluster_id = graph_clustering(num)\n",
    "    \n",
    "    # 각 클러스터에 해당하는 개수\n",
    "    length = [1]*num\n",
    "    for i in range(num):\n",
    "        length[i] += len(cluster_id[cluster_id.cluster==i]) \n",
    "    \n",
    "    \n",
    "    # train, test 에 cluster 정보 추가\n",
    "    user_item_train_cl = pd.concat([train_matrix, cluster_id], axis=1, join='inner')\n",
    "    user_item_test_cl = pd.concat([test_matrix, cluster_id], axis=1, join='inner')\n",
    "    \n",
    "    ## 2. 클러스터 별로 각 item의 mean 값 구함 (train 대상)\n",
    "    mean_rating = pd.DataFrame(columns = user_item_train_cl.columns)\n",
    "    mean_rating.set_index('cluster')\n",
    "    \n",
    "    for i in range(num):\n",
    "        mean_rating = mean_rating.append(user_item_train_cl[user_item_train_cl.cluster == i].mean(axis=0), ignore_index=True)\n",
    "    \n",
    "    mean_rating = mean_rating.set_index('cluster')\n",
    "    mean_rating\n",
    "    \n",
    "    ## multiuclass 1~5점으로 반올림하여 다중 분류 생성\n",
    "    for i in mean_rating.index:\n",
    "        for j in mean_rating.columns:\n",
    "             mean_rating.loc[i][j] = round(mean_rating.loc[i][j])\n",
    "\n",
    "    user_item_test_cl = user_item_test_cl.round(0)\n",
    "    \n",
    "\n",
    "    ## 3. train-test set의 columns(item id) 맞추기 (miss matching 제거)\n",
    "    for c in user_item_train_cl.columns:\n",
    "        if c not in user_item_test_cl.columns:\n",
    "            del mean_rating[c]\n",
    "        \n",
    "    for c in user_item_test_cl.columns:\n",
    "        if c not in user_item_train_cl.columns:\n",
    "            del user_item_test_cl[c] \n",
    "            \n",
    "    y_pred = mean_rating \n",
    "    y_true = user_item_test_cl\n",
    "    \n",
    "    \n",
    "    pre_result = [0]*num # 결과값 저장 리스트\n",
    "    rec_result = [0]*num\n",
    "    \n",
    "    ## 4. 각 결과 값에 nDCG 더해줌\n",
    "    for idx in test_matrix.index:\n",
    "        cluster_num = int(y_true.loc[idx].cluster)\n",
    "        pre_result[cluster_num] += precision_score(list(y_true.loc[idx][:-1].values) ,list(y_pred.loc[cluster_num].values ), average = 'macro')\n",
    "        rec_result[cluster_num] += recall_score(list(y_true.loc[idx][:-1].values) ,list(y_pred.loc[cluster_num].values ), average = 'macro')\n",
    "    \n",
    "    print(pre_result)\n",
    "    print(rec_result)\n",
    "    #print(sum(pre_result)/num)\n",
    "    \n",
    "    #print(sum(rec_result)/num)\n",
    "    ## 5. 최종적으로 각 precision, recall값 / 각 cluster의 요소 개수\n",
    "    #for i in range(num):\n",
    "    #    result[i] = result[i]/length[i]\n",
    "\n",
    "    \n",
    "    #print(\"cluster수:\",len(length),\"/ NDCG:\",sum(result)/len(length))  \n",
    "    print('%.5f'%(sum(pre_result)/num), \", \",  '%.5f'%(sum(rec_result)/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89da069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cluster_num = range(2,31)\n",
    "#cluster_num=[2]\n",
    "\n",
    "for i in cluster_num:\n",
    "    #reduced_total = SVD.fit_transform(user_item_total)\n",
    "    #reduced_total = pd.DataFrame(reduced_total)\n",
    "    #reduced_total.index = total_user_idx\n",
    "    \n",
    "    #user_item_train = pd.DataFrame(user_item_train)\n",
    "    \n",
    "    groupRS_pre_rec2(i, user_item_total, user_item_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4d6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e58f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ce13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e42acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881aaad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
